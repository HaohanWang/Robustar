{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Robustar Interactive Toolbox for Precise Data Annotation and Robust Vision Learning, which aims to improve the robustness of vision classification machine learning models through a data-driven perspective. Building upon the recent understanding that the lack of machine learning model's robustness is the tendency of the model's learning of spurious features, we aim to solve this problem from its root at the data perspective by removing the spurious features from the data before training. In particular, we introduce a software that helps the users to better prepare the data for training image classification models by allowing the users to annotate the spurious features at the pixel level of images. To facilitate this process, our software also leverages recent advances to help identify potential images and pixels worthy of attention and to continue the training with newly annotated data.","title":"Home"},{"location":"#welcome-to-robustar","text":"Interactive Toolbox for Precise Data Annotation and Robust Vision Learning, which aims to improve the robustness of vision classification machine learning models through a data-driven perspective. Building upon the recent understanding that the lack of machine learning model's robustness is the tendency of the model's learning of spurious features, we aim to solve this problem from its root at the data perspective by removing the spurious features from the data before training. In particular, we introduce a software that helps the users to better prepare the data for training image classification models by allowing the users to annotate the spurious features at the pixel level of images. To facilitate this process, our software also leverages recent advances to help identify potential images and pixels worthy of attention and to continue the training with newly annotated data.","title":"Welcome to Robustar"},{"location":"get_started/","text":"Setup In progress...","title":"Get Started"},{"location":"get_started/#setup","text":"In progress...","title":"Setup"},{"location":"developer/developer_guide/","text":"Developer Guide We will go through the implementation details for Robustar. For environment setup issues, please follow the frontend setup guide and backend setup guide in our repository. Robustar Architecture Robustar's architecture is shown below. Robustar is developed as a Web application wrapped in a docker image. We use Web stack mainly because of the following reasons: With Web GUI, we don't need to worry about cross platform issues. There is more abundant resources for developing good visual components with Web. With a separate frontend and backend, it is easier to refactor Robustar if we want to deploy it on the cloud in a SaaS manner in the future. For now, users will need to pull the docker image and host the frontend and backend on their own machines. However, we hide the docker layer away with a launcher. They can simply download the launcher, click a few buttons on the GUI, and then use a browser to use Robustar. The launcher is implemented in Python with pyside2 (something similar to PyQT ). It can be compiled for multiple systems. We will talk about the launcher later. The Frontend Here is a list of the main stack we used for the frontend. Library / Tool Usage HTML/CSS, JavaScript, Vue.js Web basics & frontend framework Lerna Monorepo management Vuetify Visual component library eslint & prettier Code Styling Axios API client Cypress Unit tests & end-to-end tests Besides, we adopted tui-image-editor to suit our image annotation needs and placed it in our repository. Thus, we have two separate packages for the frontend: image-editor package for tui-image-editor and robustar for the main frontend. robustar package relies on image-editor , and this is managed by lerna . The main Robustar interface is in /front-end/packages/robustar/src . These are the most important folders: views/ : Each file corresponds to a page that can be accessed with a different URL. components/ : Stores visual components, for example, model visualizers, that can be used in multiple pages. services/ : Stores all API call utilities. For tui-image-editor , please visit their GitHub page . The Backend Here are the main backend libraries we used: Library / Tool Usage Flask API server in Python python-socketio Socket connection utility SQLite Database Flashtorch Model saliency map visualization pytorch_influence_functions Influence function calculation Pillow Python's image library PyTorch, torchvision, numpy, scipy Machine learning pipeline utilities Tensorboard Model training visualization pytest Unit test The backend is organized as follows: apis/ : All API endpoint definitions ml/ : All code related to the machine learning pipeline (e.g., dataset definition, influence calculation, trainer initialization, ...) modules/ : Independent modules (usually code adopted from other places) bg_remove/ : For auto-removing the background of images with a image segmentation model influence_module/ : For influence function calculations visualize_module : For saliency map visualization (with flashtorch ) objects/ : All important Python Classes RAutoAnnotator : A wrapper for image segmentation model for background removal RDataManager : Represents the entire dataset (all splits + paired data). A aggregation of RImageFolders . RImageFolder : Represents a data split. Supports data manipulation and interacts with the SQLite database. RModelWrapper : A wrapper for models that the user is training / interacting with. RServer : Represents the entire server instance. Stores useful global information. RTask : Represents a task (e.g., model training, batch auto-annotation, testing...), which usually takes some time to finish. Can be started / updated / stopped. utils/ : Implementation of APIs. tests/ : Test cases. Backend File System Interaction The backend will read from / write to the local storage inside the container. Specifically: It will initialize a SQLite database if it does not exist. All metadata will be stored in the database. These include: All data records All visualization records All influence records All correctly/incorrectly classified samples It will save model's checkpoint. It will save the images annotated by the user. It will save any pre-calculated results, such as visualization images and background removal proposals. When starting the container with a launcher, users can mount these files from their file system into the container, so that any changes will be reflected in their file system. Inside the container, the backend will access these files with hard coded paths, which are available in RServer object. CI/CD & Tests Backend tests are available in back-end/tests . Frontend tests are available in front-end/cypress/tests/components . Refer to frontend and backend setup guide in our repo to run these tests locally. We use CircleCI to perform CI/CD tasks. The configuration can be found in .circleci/config.yml . We do the following in the pipeline Pull code from GitHub Download dependencies and dev dataset Run all test cases Build docker image Push docker image to DockerHub If the commit is made to the main branch, we will trigger a release build, which has a tag of the format x.y.z . Otherwise, when the any commit is pushed to other branches, we trigger a dev build with tag x.y.<commit_hash> . Docker We build a docker image in two steps. First, we will build a base image with /docker-base/DOCKERFILE , which will include all necessary libraries and tools ready, except for everything related to PyTorch. Then, based on the base image, we then build the full image with /DOCKERFILE . Note that we can supply an argument specifying the CUDA version to build for, and the script /scripts/install_pytorch.sh will install the corresponding version. Launcher","title":"System overview"},{"location":"developer/developer_guide/#developer-guide","text":"We will go through the implementation details for Robustar. For environment setup issues, please follow the frontend setup guide and backend setup guide in our repository.","title":"Developer Guide"},{"location":"developer/developer_guide/#robustar-architecture","text":"Robustar's architecture is shown below. Robustar is developed as a Web application wrapped in a docker image. We use Web stack mainly because of the following reasons: With Web GUI, we don't need to worry about cross platform issues. There is more abundant resources for developing good visual components with Web. With a separate frontend and backend, it is easier to refactor Robustar if we want to deploy it on the cloud in a SaaS manner in the future. For now, users will need to pull the docker image and host the frontend and backend on their own machines. However, we hide the docker layer away with a launcher. They can simply download the launcher, click a few buttons on the GUI, and then use a browser to use Robustar. The launcher is implemented in Python with pyside2 (something similar to PyQT ). It can be compiled for multiple systems. We will talk about the launcher later.","title":"Robustar Architecture"},{"location":"developer/developer_guide/#the-frontend","text":"Here is a list of the main stack we used for the frontend. Library / Tool Usage HTML/CSS, JavaScript, Vue.js Web basics & frontend framework Lerna Monorepo management Vuetify Visual component library eslint & prettier Code Styling Axios API client Cypress Unit tests & end-to-end tests Besides, we adopted tui-image-editor to suit our image annotation needs and placed it in our repository. Thus, we have two separate packages for the frontend: image-editor package for tui-image-editor and robustar for the main frontend. robustar package relies on image-editor , and this is managed by lerna . The main Robustar interface is in /front-end/packages/robustar/src . These are the most important folders: views/ : Each file corresponds to a page that can be accessed with a different URL. components/ : Stores visual components, for example, model visualizers, that can be used in multiple pages. services/ : Stores all API call utilities. For tui-image-editor , please visit their GitHub page .","title":"The Frontend"},{"location":"developer/developer_guide/#the-backend","text":"Here are the main backend libraries we used: Library / Tool Usage Flask API server in Python python-socketio Socket connection utility SQLite Database Flashtorch Model saliency map visualization pytorch_influence_functions Influence function calculation Pillow Python's image library PyTorch, torchvision, numpy, scipy Machine learning pipeline utilities Tensorboard Model training visualization pytest Unit test The backend is organized as follows: apis/ : All API endpoint definitions ml/ : All code related to the machine learning pipeline (e.g., dataset definition, influence calculation, trainer initialization, ...) modules/ : Independent modules (usually code adopted from other places) bg_remove/ : For auto-removing the background of images with a image segmentation model influence_module/ : For influence function calculations visualize_module : For saliency map visualization (with flashtorch ) objects/ : All important Python Classes RAutoAnnotator : A wrapper for image segmentation model for background removal RDataManager : Represents the entire dataset (all splits + paired data). A aggregation of RImageFolders . RImageFolder : Represents a data split. Supports data manipulation and interacts with the SQLite database. RModelWrapper : A wrapper for models that the user is training / interacting with. RServer : Represents the entire server instance. Stores useful global information. RTask : Represents a task (e.g., model training, batch auto-annotation, testing...), which usually takes some time to finish. Can be started / updated / stopped. utils/ : Implementation of APIs. tests/ : Test cases.","title":"The Backend"},{"location":"developer/developer_guide/#backend-file-system-interaction","text":"The backend will read from / write to the local storage inside the container. Specifically: It will initialize a SQLite database if it does not exist. All metadata will be stored in the database. These include: All data records All visualization records All influence records All correctly/incorrectly classified samples It will save model's checkpoint. It will save the images annotated by the user. It will save any pre-calculated results, such as visualization images and background removal proposals. When starting the container with a launcher, users can mount these files from their file system into the container, so that any changes will be reflected in their file system. Inside the container, the backend will access these files with hard coded paths, which are available in RServer object.","title":"Backend File System Interaction"},{"location":"developer/developer_guide/#cicd-tests","text":"Backend tests are available in back-end/tests . Frontend tests are available in front-end/cypress/tests/components . Refer to frontend and backend setup guide in our repo to run these tests locally. We use CircleCI to perform CI/CD tasks. The configuration can be found in .circleci/config.yml . We do the following in the pipeline Pull code from GitHub Download dependencies and dev dataset Run all test cases Build docker image Push docker image to DockerHub If the commit is made to the main branch, we will trigger a release build, which has a tag of the format x.y.z . Otherwise, when the any commit is pushed to other branches, we trigger a dev build with tag x.y.<commit_hash> .","title":"CI/CD &amp; Tests"},{"location":"developer/developer_guide/#docker","text":"We build a docker image in two steps. First, we will build a base image with /docker-base/DOCKERFILE , which will include all necessary libraries and tools ready, except for everything related to PyTorch. Then, based on the base image, we then build the full image with /DOCKERFILE . Note that we can supply an argument specifying the CUDA version to build for, and the script /scripts/install_pytorch.sh will install the corresponding version.","title":"Docker"},{"location":"developer/developer_guide/#launcher","text":"","title":"Launcher"},{"location":"tutorial/launcher/launcher/","text":"Robustar Launcher In progress...","title":"Launcher"},{"location":"tutorial/launcher/launcher/#robustar-launcher","text":"In progress...","title":"Robustar Launcher"},{"location":"tutorial/web/annotation/","text":"Annotation of Spurious Pixel Features Inspect data - Training data Navigation PREV PAGE - Go to the previous page GOTO PAGE - Go to a certain page NEXT PAGE - Go to the next page GOTO CLASS - Go to a certain class Display Enter the number of images per page Select the image size Annotate Hover on an image Click on the ANNOTATE button Adjust Size of the image Load Edit - load your previous annotation Auto Edit - directly filter out all the background pixels Use Free pencil to brush out the pixels that are considered spurious, adjust the brush size with Range Use Color Range to filter out all the background pixels with certain color range Auto Edit with Built-in segmentation mode Send Edit View edit results Allow annotators to calibrate the perception process of the vision models at a pixel level. The users can user canvas tool to annotate the spurious pixels of the images.","title":"Annotation"},{"location":"tutorial/web/annotation/#annotation-of-spurious-pixel-features","text":"Inspect data - Training data","title":"Annotation of Spurious Pixel Features"},{"location":"tutorial/web/annotation/#navigation","text":"PREV PAGE - Go to the previous page GOTO PAGE - Go to a certain page NEXT PAGE - Go to the next page GOTO CLASS - Go to a certain class","title":"Navigation"},{"location":"tutorial/web/annotation/#display","text":"Enter the number of images per page Select the image size","title":"Display"},{"location":"tutorial/web/annotation/#annotate","text":"Hover on an image Click on the ANNOTATE button Adjust Size of the image Load Edit - load your previous annotation Auto Edit - directly filter out all the background pixels Use Free pencil to brush out the pixels that are considered spurious, adjust the brush size with Range Use Color Range to filter out all the background pixels with certain color range Auto Edit with Built-in segmentation mode Send Edit View edit results Allow annotators to calibrate the perception process of the vision models at a pixel level. The users can user canvas tool to annotate the spurious pixels of the images.","title":"Annotate"},{"location":"tutorial/web/auto_annotation/","text":"Auto Annotation Enter the start index of samples Enter the end index of sample ( -1 means the end of all samples) Click START AUTO ANNOTATION You can annotate a certain set of images automatically.","title":"Auto Annotation"},{"location":"tutorial/web/auto_annotation/#auto-annotation","text":"Enter the start index of samples Enter the end index of sample ( -1 means the end of all samples) Click START AUTO ANNOTATION You can annotate a certain set of images automatically.","title":"Auto Annotation"},{"location":"tutorial/web/influence/","text":"Influence Input number of test samples Input r-averaging Start calculation You can use influence function to help identify the samples that need attention.","title":"Influence"},{"location":"tutorial/web/influence/#influence","text":"Input number of test samples Input r-averaging Start calculation You can use influence function to help identify the samples that need attention.","title":"Influence"},{"location":"tutorial/web/prediction/","text":"View model prediction Inspect Data - Annotated Data Hover on an image Click on the Predict button","title":"View model prediction"},{"location":"tutorial/web/prediction/#view-model-prediction","text":"Inspect Data - Annotated Data Hover on an image Click on the Predict button","title":"View model prediction"},{"location":"tutorial/web/task/","text":"Task center Train With annotation of spurious features all misleading samples, the users can directly use our system to update the model to improve its robustness against its tendency in learning the spurious features through our task center.","title":"Task Center"},{"location":"tutorial/web/task/#task-center","text":"Train With annotation of spurious features all misleading samples, the users can directly use our system to update the model to improve its robustness against its tendency in learning the spurious features through our task center.","title":"Task center"}]}