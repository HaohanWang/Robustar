{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Robustar Interactive Toolbox for Precise Data Annotation and Robust Vision Learning, which aims to improve the robustness of vision classification machine learning models through a data-driven perspective. Building upon the recent understanding that the lack of machine learning model's robustness is the tendency of the model's learning of spurious features, we aim to solve this problem from its root at the data perspective by removing the spurious features from the data before training. In particular, we introduce a software that helps the users to better prepare the data for training image classification models by allowing the users to annotate the spurious features at the pixel level of images. To facilitate this process, our software also leverages recent advances to help identify potential images and pixels worthy of attention and to continue the training with newly annotated data.","title":"Home"},{"location":"#welcome-to-robustar","text":"Interactive Toolbox for Precise Data Annotation and Robust Vision Learning, which aims to improve the robustness of vision classification machine learning models through a data-driven perspective. Building upon the recent understanding that the lack of machine learning model's robustness is the tendency of the model's learning of spurious features, we aim to solve this problem from its root at the data perspective by removing the spurious features from the data before training. In particular, we introduce a software that helps the users to better prepare the data for training image classification models by allowing the users to annotate the spurious features at the pixel level of images. To facilitate this process, our software also leverages recent advances to help identify potential images and pixels worthy of attention and to continue the training with newly annotated data.","title":"Welcome to Robustar"},{"location":"get_started/","text":"Setup In progress...","title":"Get Started"},{"location":"get_started/#setup","text":"In progress...","title":"Setup"},{"location":"developer/developer_guide/","text":"Developer Guide We will go through the implementation details for Robustar. For environment setup issues, please follow the frontend setup guide , backend setup guide and launcher setup guide in our repository. Robustar Architecture Robustar's architecture is shown below. Robustar is developed as a Web application wrapped in a docker image. We use Web stack mainly because of the following reasons: With Web GUI, we don't need to worry about cross platform issues. There is more abundant resources for developing good visual components with Web. With a separate frontend and backend, it is easier to refactor Robustar if we want to deploy it on the cloud in a SaaS manner in the future. For now, users will need to pull the docker image and host the frontend and backend on their own machines. However, we hide the docker layer away with a launcher. They can simply download the launcher, click a few buttons on the GUI, and then use a browser to use Robustar. The launcher is implemented in Python with pyside2 (something similar to PyQT ). It can be compiled for multiple systems. We will talk about the launcher later. The Frontend Here is a list of the main stack we used for the frontend. Library / Tool Usage HTML/CSS, JavaScript, Vue.js Web basics & frontend framework Lerna Monorepo management Vuetify Visual component library eslint & prettier Code Styling Axios API client Cypress Unit tests & end-to-end tests Besides, we adopted tui-image-editor to suit our image annotation needs and placed it in our repository. Thus, we have two separate packages for the frontend: image-editor package for tui-image-editor and robustar for the main frontend. robustar package relies on image-editor , and this is managed by lerna . The main Robustar interface is in /front-end/packages/robustar/src . These are the most important folders: views/ : Each file corresponds to a page that can be accessed with a different URL. components/ : Stores visual components, for example, model visualizers, that can be used in multiple pages. services/ : Stores all API call utilities. For tui-image-editor , please visit their GitHub page . The Backend Here are the main backend libraries we used: Library / Tool Usage Flask API server in Python python-socketio Socket connection utility SQLite Database Flashtorch Model saliency map visualization pytorch_influence_functions Influence function calculation Pillow Python's image library PyTorch, torchvision, numpy, scipy Machine learning pipeline utilities Tensorboard Model training visualization pytest Unit test The backend is organized as follows: apis/ : All API endpoint definitions ml/ : All code related to the machine learning pipeline (e.g., dataset definition, influence calculation, trainer initialization, ...) modules/ : Independent modules (usually code adopted from other places) bg_remove/ : For auto-removing the background of images with a image segmentation model influence_module/ : For influence function calculations visualize_module : For saliency map visualization (with flashtorch ) objects/ : All important Python Classes RAutoAnnotator : A wrapper for image segmentation model for background removal RDataManager : Represents the entire dataset (all splits + paired data). A aggregation of RImageFolders . RImageFolder : Represents a data split. Supports data manipulation and interacts with the SQLite database. RModelWrapper : A wrapper for models that the user is training / interacting with. RServer : Represents the entire server instance. Stores useful global information. RTask : Represents a task (e.g., model training, batch auto-annotation, testing...), which usually takes some time to finish. Can be started / updated / stopped. utils/ : Implementation of APIs. tests/ : Test cases. Backend File System Interaction The backend will read from / write to the local storage inside the container. Specifically: It will initialize a SQLite database if it does not exist. All metadata will be stored in the database. These include: All data records All visualization records All influence records All correctly/incorrectly classified samples It will save model's checkpoint. It will save the images annotated by the user. It will save any pre-calculated results, such as visualization images and background removal proposals. When starting the container with a launcher, users can mount these files from their file system into the container, so that any changes will be reflected in their file system. Inside the container, the backend will access these files with hard coded paths, which are available in RServer object. CI/CD & Tests Backend tests are available in back-end/tests . Frontend tests are available in front-end/cypress/tests/components . Refer to frontend and backend setup guide in our repo to run these tests locally. We use CircleCI to perform CI/CD tasks. The configuration can be found in .circleci/config.yml . We do the following in the pipeline Pull code from GitHub Download dependencies and dev dataset Run all test cases Build docker image Push docker image to DockerHub If the commit is made to the main branch, we will trigger a release build, which has a tag of the format x.y.z . Otherwise, when the any commit is pushed to other branches, we trigger a dev build with tag x.y.<commit_hash> . Docker We build a docker image in two steps. First, we will build a base image with /docker-base/DOCKERFILE , which will include all necessary libraries and tools ready, except for everything related to PyTorch. Then, based on the base image, we then build the full image with /DOCKERFILE . Note that we can supply an argument specifying the CUDA version to build for, and the script /scripts/install_pytorch.sh will install the corresponding version. Launcher Here are the main launcher libraries and development tools we used: Library / Tool Usage PySide2 Python\u2019s QT library docker Python\u2019s Docker library Qt Designer GUI QT development tool PyUIC Tool that converts .ui files to .py files The launcher is developed adhering to Model\u2013View\u2013Controller(MVC) pattern and is organized as follows: app.py : Main file of the launcher. model/ : The launcher model. Each property is defined with the decoration of @property and its corresponding setter function is defined with @<property name>.setter . resources/ : All .ui files developed with Qt Designer . views/ : All view files in pairs. Specifically, xx_view_ui.py generated directly from a .ui file with PyUIC pairs with xx_view.py , which connects signals in the view to corresponding controller functions. controllers/ : All controller classes main_ctrl.py : Contains MainController class, including functions controlling the general behavior of the launcher. Two main types of functions shall be distinguished, functions beginning with setM and setV . The former ones are functions called by views to change the model, and the latter ones are functions called by the model to change the view. docker_ctrl.py : Contains DockerController class, including docker relevant operation functions. \u200b To package the launcher, please follow the launcher setup guide in our repository.","title":"System overview"},{"location":"developer/developer_guide/#developer-guide","text":"We will go through the implementation details for Robustar. For environment setup issues, please follow the frontend setup guide , backend setup guide and launcher setup guide in our repository.","title":"Developer Guide"},{"location":"developer/developer_guide/#robustar-architecture","text":"Robustar's architecture is shown below. Robustar is developed as a Web application wrapped in a docker image. We use Web stack mainly because of the following reasons: With Web GUI, we don't need to worry about cross platform issues. There is more abundant resources for developing good visual components with Web. With a separate frontend and backend, it is easier to refactor Robustar if we want to deploy it on the cloud in a SaaS manner in the future. For now, users will need to pull the docker image and host the frontend and backend on their own machines. However, we hide the docker layer away with a launcher. They can simply download the launcher, click a few buttons on the GUI, and then use a browser to use Robustar. The launcher is implemented in Python with pyside2 (something similar to PyQT ). It can be compiled for multiple systems. We will talk about the launcher later.","title":"Robustar Architecture"},{"location":"developer/developer_guide/#the-frontend","text":"Here is a list of the main stack we used for the frontend. Library / Tool Usage HTML/CSS, JavaScript, Vue.js Web basics & frontend framework Lerna Monorepo management Vuetify Visual component library eslint & prettier Code Styling Axios API client Cypress Unit tests & end-to-end tests Besides, we adopted tui-image-editor to suit our image annotation needs and placed it in our repository. Thus, we have two separate packages for the frontend: image-editor package for tui-image-editor and robustar for the main frontend. robustar package relies on image-editor , and this is managed by lerna . The main Robustar interface is in /front-end/packages/robustar/src . These are the most important folders: views/ : Each file corresponds to a page that can be accessed with a different URL. components/ : Stores visual components, for example, model visualizers, that can be used in multiple pages. services/ : Stores all API call utilities. For tui-image-editor , please visit their GitHub page .","title":"The Frontend"},{"location":"developer/developer_guide/#the-backend","text":"Here are the main backend libraries we used: Library / Tool Usage Flask API server in Python python-socketio Socket connection utility SQLite Database Flashtorch Model saliency map visualization pytorch_influence_functions Influence function calculation Pillow Python's image library PyTorch, torchvision, numpy, scipy Machine learning pipeline utilities Tensorboard Model training visualization pytest Unit test The backend is organized as follows: apis/ : All API endpoint definitions ml/ : All code related to the machine learning pipeline (e.g., dataset definition, influence calculation, trainer initialization, ...) modules/ : Independent modules (usually code adopted from other places) bg_remove/ : For auto-removing the background of images with a image segmentation model influence_module/ : For influence function calculations visualize_module : For saliency map visualization (with flashtorch ) objects/ : All important Python Classes RAutoAnnotator : A wrapper for image segmentation model for background removal RDataManager : Represents the entire dataset (all splits + paired data). A aggregation of RImageFolders . RImageFolder : Represents a data split. Supports data manipulation and interacts with the SQLite database. RModelWrapper : A wrapper for models that the user is training / interacting with. RServer : Represents the entire server instance. Stores useful global information. RTask : Represents a task (e.g., model training, batch auto-annotation, testing...), which usually takes some time to finish. Can be started / updated / stopped. utils/ : Implementation of APIs. tests/ : Test cases.","title":"The Backend"},{"location":"developer/developer_guide/#backend-file-system-interaction","text":"The backend will read from / write to the local storage inside the container. Specifically: It will initialize a SQLite database if it does not exist. All metadata will be stored in the database. These include: All data records All visualization records All influence records All correctly/incorrectly classified samples It will save model's checkpoint. It will save the images annotated by the user. It will save any pre-calculated results, such as visualization images and background removal proposals. When starting the container with a launcher, users can mount these files from their file system into the container, so that any changes will be reflected in their file system. Inside the container, the backend will access these files with hard coded paths, which are available in RServer object.","title":"Backend File System Interaction"},{"location":"developer/developer_guide/#cicd-tests","text":"Backend tests are available in back-end/tests . Frontend tests are available in front-end/cypress/tests/components . Refer to frontend and backend setup guide in our repo to run these tests locally. We use CircleCI to perform CI/CD tasks. The configuration can be found in .circleci/config.yml . We do the following in the pipeline Pull code from GitHub Download dependencies and dev dataset Run all test cases Build docker image Push docker image to DockerHub If the commit is made to the main branch, we will trigger a release build, which has a tag of the format x.y.z . Otherwise, when the any commit is pushed to other branches, we trigger a dev build with tag x.y.<commit_hash> .","title":"CI/CD &amp; Tests"},{"location":"developer/developer_guide/#docker","text":"We build a docker image in two steps. First, we will build a base image with /docker-base/DOCKERFILE , which will include all necessary libraries and tools ready, except for everything related to PyTorch. Then, based on the base image, we then build the full image with /DOCKERFILE . Note that we can supply an argument specifying the CUDA version to build for, and the script /scripts/install_pytorch.sh will install the corresponding version.","title":"Docker"},{"location":"developer/developer_guide/#launcher","text":"Here are the main launcher libraries and development tools we used: Library / Tool Usage PySide2 Python\u2019s QT library docker Python\u2019s Docker library Qt Designer GUI QT development tool PyUIC Tool that converts .ui files to .py files The launcher is developed adhering to Model\u2013View\u2013Controller(MVC) pattern and is organized as follows: app.py : Main file of the launcher. model/ : The launcher model. Each property is defined with the decoration of @property and its corresponding setter function is defined with @<property name>.setter . resources/ : All .ui files developed with Qt Designer . views/ : All view files in pairs. Specifically, xx_view_ui.py generated directly from a .ui file with PyUIC pairs with xx_view.py , which connects signals in the view to corresponding controller functions. controllers/ : All controller classes main_ctrl.py : Contains MainController class, including functions controlling the general behavior of the launcher. Two main types of functions shall be distinguished, functions beginning with setM and setV . The former ones are functions called by views to change the model, and the latter ones are functions called by the model to change the view. docker_ctrl.py : Contains DockerController class, including docker relevant operation functions. \u200b To package the launcher, please follow the launcher setup guide in our repository.","title":"Launcher"},{"location":"tutorial/launcher/launcher/","text":"Introduction Robustar Launcher is an interactive application that enables you to control Docker containers for Robustar. You can download and run the executable file from here or use the provided code to launch the application. At present, our software only supports the Windows operating system. However, we are planning to expand our support to include other operating systems such as Linux and Mac in the near future. With Robustar Launcher, you can easily manage and control your Docker containers, making it a powerful tool for your machine learning workflow. Contents Overview of Robustar Launcher Start Robustar Launcher Use Robustar Launcher Create a new container Manage present containers Check Launcher outputs Appendix Prerequisites to run Robustar Launcher Frequently asked questions 1. Overview of Robustar Launcher Robustar Launcher is shown below. The figure above consists of two major parts: the upper part (denoted by a ) is for operations and the lower part (denoted by b ) is for displaying information. Part a contains two tab pages: The Create tab page, which is used for creating a new server (i.e., container). The Manage tab page, which is used for managing existing servers. Part b contains three tab pages: The Prompts tab page, which displays essential prompts. The Details tab page, which displays more detailed information such as detailed errors and Docker image download progress. The Logs tab page, which displays the complete container logs. 2. Start Robustar Launcher To start Robustar Launcher, you can either run the executable file or the source code . If you want to run the source code, make sure that you have the prerequisites satis\ufb01ed. See here for more information. If you start using the executable file: For Windows: Just double click the software. For Linux: right-click the software and select \"Run.\" Otherwise, you will need to execute sudo ./\"Robustar Launcher\" in the terminal. (NOTE: Please replace ./\"Robustar Launcher\" with the appropriate path to the software on your machine.) If you start using the source code, first make you sure you are in the launcher/ directory: For Windows: run python app.py in the terminal. For Linux: If your account has permission to directly fetch the Docker server API version, you can run python app.py in the terminal. Otherwise, you will need to elevate your privileges by running sudo -i first and then execute python app.py . 3. Use Robustar Launcher Here is an outline of the various functions of Robustar Launcher: Create a new container Manage present containers Check launcher outputs Detailed instructions of these functions will be provided in turn below. 3.a. Create a new container To create a new container, follow these steps: Navigate to the Create tab page. Complete all the required input fields. Click on the Create Server button. Monitor the status update on the Prompts tab page to confirm if the creation process was successful. Here are specific instructions for completing each input field: Container Name : Enter a name for the container. Avoid using the same name as an existing container. Image Version : Select the version of the Docker image to be used for the container. Port : Enter the port number where the software's webpage will be hosted and can be accessed for interaction. Train/Validation/Test/Paired Set : Select the root path of the train/validation/test/paired dataset. Checkpoint : Select the root path where the model checkpoint files will be stored. Files with the .pth or .pt extension in this path will be provided as options in the Weight File field. The checkpoint files generated during the training process will also be stored here. Influence Result : Select the root path where the influence calculation results will be stored. Output Folder : Select the root path where other generated files, such as the database file, will be stored. Architecture : Select the architecture of the model you want to create. Pretrained : Choose whether to use the pretrained weight (provided by torchvision ) of the model. Weight File : Select the checkpoint file to be used for initializing the weights of the model. Available options are the files with the .pth or .pt extension in the path set in the Checkpoint field. If you don't want to use a checkpoint file for initialization, select None . If you choose Pretrained and select a checkpoint file simultaneously, the model will adhere to the weights in the checkpoint file. Device : Select the device to run the container. Shuffle : Choose whether to shuffle the train set. Batch Size : Enter the batch size for the train, test, and validation data loader. Worker Number : Enter the number of workers for the data loader. Class Number : Enter the number of classes of the dataset. Image Size : Enter the length of the sides of the transformed square image to be inputted into the model. Image Padding : Choose the mode to be used for padding the image as a square. To save time configuring a new container in the future, you can save the current configuration as a JSON file by clicking on the Save Profile button. This saves the current configuration profile for future use. The next time you want to use the same configuration, simply click on the Load Profile button and select the appropriate JSON file. This will load the saved configuration into the launcher, allowing you to quickly create a new container with the same settings. 3.b. Manage present containers To mange present containers, follow these steps: Navigate to the Manage tab page. Select the container to be operated on from the lists of present containers. Click on the Start Server / Stop Server / Delete Server button. Monitor the status update on the Prompts tab page to confirm if the operation was successful. Present containers are categorized into three categories, according to their status: Running : Containers that are currently running. Exited : Containers that have exited from the Running state. Created : Containers that were created but never had the chance to run. The Launcher now passively updates the status of the containers. When an operation is conducted successfully, or when you switch from the Create tab page to the Manage tab page, the Launcher will update the status of the containers. However, if you remain on the Manage tab page and the status of a container changes due to reasons other than an operation, the update of its status will not be displayed. In such a case, you can manually force a status update by clicking on the Refresh button. This will display the most up-to-date status information for all present containers. 3.c. Check Launcher outputs There are three tab pages in the Launcher where you can view the output at different levels: the Prompts tab page, the Details tab page, and the Logs tab page. In the Prompts tab page, you can view the most essential information, such as whether an operation has been carried out successfully. This tab page provides a summary of the most important information related to the current operation. In the Details tab page, you can view more detailed information about the current operation, including detailed errors and progress updates for Docker image downloads. This tab page provides a more comprehensive view of the operation, allowing you to troubleshoot issues and monitor progress in more detail. In the Logs tab page, you can view the real-time logs of the containers. Because multiple containers may be running at the same time, an identifier of the format name(port) is inserted after the time stamp for each log. Currently, the Launcher only automatically displays the logs of containers that were started during the current session. Logs for containers started before launching the Launcher will not be displayed automatically. However, if you start a new container, the logs of both the newly created container and any previously started containers that are running will be displayed in real-time. 4. Appendix 4.a. Prerequisites to run Robustar Launcher To run Robustar Launcher, you must have Docker Engine installed and running on your computer. This applies to both the executable file and the code. If you're using Windows, ensure that you've exposed the daemon on tcp://localhost:2375 without TLS. Shown below is how you can enable this using Docker Desktop: If you want to run the Launcher from the source code , you'll need to install the necessary Python packages. To do this, navigate to the launcher/ directory and run pip install -r requirements.txt in the command prompt. 4.b. Frequently asked questions","title":"Launcher"},{"location":"tutorial/launcher/launcher/#introduction","text":"Robustar Launcher is an interactive application that enables you to control Docker containers for Robustar. You can download and run the executable file from here or use the provided code to launch the application. At present, our software only supports the Windows operating system. However, we are planning to expand our support to include other operating systems such as Linux and Mac in the near future. With Robustar Launcher, you can easily manage and control your Docker containers, making it a powerful tool for your machine learning workflow.","title":"Introduction"},{"location":"tutorial/launcher/launcher/#contents","text":"Overview of Robustar Launcher Start Robustar Launcher Use Robustar Launcher Create a new container Manage present containers Check Launcher outputs Appendix Prerequisites to run Robustar Launcher Frequently asked questions","title":"Contents"},{"location":"tutorial/web/annotation/","text":"Annotation of Spurious Pixel Features Inspect data - Training data Navigation PREV PAGE - Go to the previous page GOTO PAGE - Go to a certain page NEXT PAGE - Go to the next page GOTO CLASS - Go to a certain class Display Enter the number of images per page Select the image size Annotate Hover on an image Click on the ANNOTATE button Adjust Size of the image Load Edit - load your previous annotation Auto Edit - directly filter out all the background pixels Use Free pencil to brush out the pixels that are considered spurious, adjust the brush size with Range Use Color Range to filter out all the background pixels with certain color range Auto Edit with Built-in segmentation mode Send Edit View edit results Allow annotators to calibrate the perception process of the vision models at a pixel level. The users can user canvas tool to annotate the spurious pixels of the images.","title":"Annotation"},{"location":"tutorial/web/annotation/#annotation-of-spurious-pixel-features","text":"Inspect data - Training data","title":"Annotation of Spurious Pixel Features"},{"location":"tutorial/web/annotation/#navigation","text":"PREV PAGE - Go to the previous page GOTO PAGE - Go to a certain page NEXT PAGE - Go to the next page GOTO CLASS - Go to a certain class","title":"Navigation"},{"location":"tutorial/web/annotation/#display","text":"Enter the number of images per page Select the image size","title":"Display"},{"location":"tutorial/web/annotation/#annotate","text":"Hover on an image Click on the ANNOTATE button Adjust Size of the image Load Edit - load your previous annotation Auto Edit - directly filter out all the background pixels Use Free pencil to brush out the pixels that are considered spurious, adjust the brush size with Range Use Color Range to filter out all the background pixels with certain color range Auto Edit with Built-in segmentation mode Send Edit View edit results Allow annotators to calibrate the perception process of the vision models at a pixel level. The users can user canvas tool to annotate the spurious pixels of the images.","title":"Annotate"},{"location":"tutorial/web/auto_annotation/","text":"Auto Annotation Enter the start index of samples Enter the end index of sample ( -1 means the end of all samples) Click START AUTO ANNOTATION You can annotate a certain set of images automatically.","title":"Auto Annotation"},{"location":"tutorial/web/auto_annotation/#auto-annotation","text":"Enter the start index of samples Enter the end index of sample ( -1 means the end of all samples) Click START AUTO ANNOTATION You can annotate a certain set of images automatically.","title":"Auto Annotation"},{"location":"tutorial/web/influence/","text":"Influence Input number of test samples Input r-averaging Start calculation You can use influence function to help identify the samples that need attention.","title":"Influence"},{"location":"tutorial/web/influence/#influence","text":"Input number of test samples Input r-averaging Start calculation You can use influence function to help identify the samples that need attention.","title":"Influence"},{"location":"tutorial/web/prediction/","text":"View model prediction Inspect Data - Annotated Data Hover on an image Click on the Predict button","title":"View model prediction"},{"location":"tutorial/web/prediction/#view-model-prediction","text":"Inspect Data - Annotated Data Hover on an image Click on the Predict button","title":"View model prediction"},{"location":"tutorial/web/task/","text":"Task center Train With annotation of spurious features all misleading samples, the users can directly use our system to update the model to improve its robustness against its tendency in learning the spurious features through our task center.","title":"Task Center"},{"location":"tutorial/web/task/#task-center","text":"Train With annotation of spurious features all misleading samples, the users can directly use our system to update the model to improve its robustness against its tendency in learning the spurious features through our task center.","title":"Task center"}]}